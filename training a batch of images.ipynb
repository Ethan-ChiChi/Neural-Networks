{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a batch of images of FashionMNIST with Forward Propagation\n",
    "## input : 10 × 1 × 28 × 28 (batch size, input channels, height, width)\n",
    "## output: 10 × 10 (batch size, number of prediction classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST'\n",
    "    ,train = True\n",
    "    , download = True\n",
    "    , transform =transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_set\n",
    "    ,batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        \n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        \n",
    "        t = F.relu(self.conv1(t))\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        \n",
    "        t = F.relu(self.conv2(t))\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n",
    "        \n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        \n",
    "        t = F.relu(self.fc1(t))\n",
    "        \n",
    "        # (5) hidden linear layer\n",
    "        \n",
    "        t = F.relu(self.fc2(t))\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t,dim = 1)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1b568904a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = batch\n",
    "images.shape #  a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0698,  0.0035, -0.0743,  0.1250, -0.0914,  0.0294, -0.0745,  0.0299,  0.0903,  0.0782],\n",
       "        [ 0.0688, -0.0031, -0.0830,  0.1218, -0.0914,  0.0256, -0.0784,  0.0295,  0.0902,  0.0774],\n",
       "        [ 0.0603,  0.0018, -0.0803,  0.1213, -0.0883,  0.0215, -0.0711,  0.0246,  0.0856,  0.0798],\n",
       "        [ 0.0615,  0.0036, -0.0794,  0.1208, -0.0900,  0.0300, -0.0766,  0.0280,  0.0892,  0.0779],\n",
       "        [ 0.0634,  0.0014, -0.0815,  0.1242, -0.0880,  0.0291, -0.0775,  0.0298,  0.0908,  0.0792],\n",
       "        [ 0.0679, -0.0050, -0.0778,  0.1197, -0.0956,  0.0374, -0.0818,  0.0283,  0.0870,  0.0733],\n",
       "        [ 0.0740,  0.0014, -0.0828,  0.1286, -0.0900,  0.0313, -0.0742,  0.0258,  0.0864,  0.0719],\n",
       "        [ 0.0708, -0.0038, -0.0763,  0.1165, -0.0929,  0.0386, -0.0820,  0.0314,  0.0909,  0.0750],\n",
       "        [ 0.0610, -0.0009, -0.0788,  0.1184, -0.0884,  0.0159, -0.0724,  0.0287,  0.0902,  0.0830],\n",
       "        [ 0.0589,  0.0056, -0.0757,  0.1230, -0.0885,  0.0120, -0.0728,  0.0305,  0.0908,  0.0858]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction tensor has a shape of 10 by 10, which gives us two axes that each have a length of ten. This reflects the fact that we have ten images and for each of these ten images we have ten prediction classes.\n",
    "\n",
    "The elements of the first dimension are arrays of length ten. Each of these array elements contain the ten predictions for each category for the corresponding image. \n",
    "\n",
    "The elements of the second dimension are numbers. Each number is the assigned value of the specific output class. The output classes are encoded by the indexes, so each index represents a specific output class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim = 1) # the prediction in this case is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1050, 0.0982, 0.0909, 0.1109, 0.0894, 0.1008, 0.0909, 0.1009, 0.1072, 0.1059],\n",
       "        [0.1052, 0.0979, 0.0904, 0.1109, 0.0896, 0.1007, 0.0908, 0.1011, 0.1074, 0.1061],\n",
       "        [0.1043, 0.0984, 0.0906, 0.1109, 0.0899, 0.1003, 0.0915, 0.1007, 0.1070, 0.1064],\n",
       "        [0.1043, 0.0985, 0.0906, 0.1107, 0.0897, 0.1011, 0.0909, 0.1009, 0.1073, 0.1061],\n",
       "        [0.1045, 0.0982, 0.0904, 0.1110, 0.0898, 0.1009, 0.0907, 0.1010, 0.1074, 0.1061],\n",
       "        [0.1051, 0.0977, 0.0909, 0.1107, 0.0893, 0.1020, 0.0905, 0.1010, 0.1071, 0.1057],\n",
       "        [0.1056, 0.0982, 0.0902, 0.1115, 0.0896, 0.1011, 0.0910, 0.1006, 0.1069, 0.1053],\n",
       "        [0.1053, 0.0977, 0.0909, 0.1102, 0.0894, 0.1019, 0.0904, 0.1012, 0.1074, 0.1057],\n",
       "        [0.1044, 0.0981, 0.0908, 0.1105, 0.0899, 0.0998, 0.0913, 0.1011, 0.1075, 0.1067],\n",
       "        [0.1040, 0.0986, 0.0909, 0.1109, 0.0898, 0.0992, 0.0912, 0.1011, 0.1074, 0.1069]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(preds,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network weights are randomly generated. Each time we create a new instance of our network, the weights within the network will be different. This means that the predictions we get will be different if we create different networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(preds, dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.1641,  0.0426, -0.1928,  0.1217, -0.0350],\n",
       "          [-0.0887, -0.0055,  0.1837, -0.0367,  0.1656],\n",
       "          [-0.0840, -0.1275, -0.1436,  0.0632, -0.1405],\n",
       "          [ 0.0735,  0.1952, -0.1149,  0.1600,  0.0350],\n",
       "          [ 0.0294,  0.0723, -0.1525,  0.1415,  0.0529]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0618,  0.1993, -0.1922,  0.0857,  0.1780],\n",
       "          [-0.1667, -0.1388, -0.0950, -0.1654,  0.0646],\n",
       "          [-0.0579, -0.1718, -0.1083,  0.1002, -0.0611],\n",
       "          [-0.1747, -0.1827, -0.0227,  0.0837,  0.1652],\n",
       "          [ 0.0329, -0.0912, -0.1391,  0.0490, -0.0896]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1830, -0.0198, -0.0146, -0.1426, -0.1560],\n",
       "          [-0.0991, -0.1411,  0.0905, -0.1715,  0.1383],\n",
       "          [-0.1089,  0.1994, -0.0440, -0.0196, -0.0010],\n",
       "          [-0.1364, -0.1437,  0.1774,  0.1472,  0.0343],\n",
       "          [ 0.0750, -0.1873,  0.1725,  0.0040, -0.1847]]],\n",
       "\n",
       "\n",
       "        [[[-0.0709,  0.0575,  0.0017, -0.0224,  0.0740],\n",
       "          [-0.1398,  0.1878, -0.0997,  0.1016,  0.1426],\n",
       "          [-0.0078,  0.0774, -0.0575, -0.1646, -0.1426],\n",
       "          [ 0.0027,  0.1593, -0.0833, -0.1415,  0.1281],\n",
       "          [ 0.0483,  0.0509,  0.1245,  0.1456, -0.1598]]],\n",
       "\n",
       "\n",
       "        [[[-0.0119, -0.0027,  0.1234, -0.0810,  0.0616],\n",
       "          [ 0.0322,  0.0532,  0.0117, -0.1694,  0.1591],\n",
       "          [-0.0084,  0.1096, -0.0012,  0.1082,  0.1131],\n",
       "          [ 0.1817,  0.0414, -0.1638,  0.1170, -0.0213],\n",
       "          [-0.1402, -0.0428,  0.0984,  0.1527, -0.0849]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1236,  0.0913,  0.0008, -0.0318,  0.1986],\n",
       "          [-0.1185, -0.1295, -0.0223,  0.1638,  0.0485],\n",
       "          [ 0.1492,  0.1657,  0.0411,  0.1853,  0.1484],\n",
       "          [-0.0546, -0.1880,  0.1141, -0.1237, -0.0071],\n",
       "          [ 0.0989, -0.1145, -0.1032, -0.0834,  0.0249]]]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-3.7620e-02, -3.0236e-02, -1.5490e-02,  ...,  5.6669e-02, -4.3223e-03,  2.2445e-02],\n",
       "        [ 4.8502e-02,  4.5177e-02,  3.0869e-02,  ...,  5.8603e-02, -1.2762e-02, -5.8258e-02],\n",
       "        [-6.1708e-03, -4.3056e-03, -6.6799e-02,  ..., -5.4304e-02,  6.5948e-03, -4.7317e-03],\n",
       "        ...,\n",
       "        [ 2.9953e-02, -3.8773e-05, -5.3876e-02,  ...,  6.1435e-02,  1.5909e-02, -5.0694e-02],\n",
       "        [-2.7693e-03, -6.3158e-02, -4.2490e-02,  ...,  1.5434e-02,  3.8789e-03,  4.5823e-03],\n",
       "        [-5.3507e-02, -1.1217e-02,  5.5563e-02,  ..., -6.7847e-02,  6.8375e-02,  6.0344e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 192])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc1.weight.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([12, 6, 5, 5])\n",
      "torch.Size([12])\n",
      "torch.Size([120, 192])\n",
      "torch.Size([120])\n",
      "torch.Size([60, 120])\n",
      "torch.Size([60])\n",
      "torch.Size([10, 60])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for para in network.parameters():\n",
    "    print(para.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t\t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t\t torch.Size([6])\n",
      "conv2.weight \t\t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t\t torch.Size([12])\n",
      "fc1.weight \t\t torch.Size([120, 192])\n",
      "fc1.bias \t\t torch.Size([120])\n",
      "fc2.weight \t\t torch.Size([60, 120])\n",
      "fc2.bias \t\t torch.Size([60])\n",
      "out.weight \t\t torch.Size([10, 60])\n",
      "out.bias \t\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, para in network.named_parameters():\n",
    "    print(name, '\\t\\t', para.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
